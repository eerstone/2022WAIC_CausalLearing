{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Application2\\anaconda\\anaconda\\envs\\ylearn-py38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "import ylearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Demo\\PythonDemo\\2022WAIC_CausalLearing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# 项目根目录\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath('./grasp_Xlearn_XGB.ipynb')))\n",
    "# 添加系统环境变量\n",
    "print(BASE_DIR)\n",
    "sys.path.insert(0, BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(BASE_DIR+'/data/train.csv')\n",
    "test = pd.read_csv(BASE_DIR+'/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "25\n",
      "2\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# replace nan\n",
    "def build_data(train):\n",
    "    train_ = {}\n",
    "    for i in train.columns:\n",
    "        # 对每一序列进行预处理\n",
    "        # preprocessing for each series\n",
    "        train_i = train[i]\n",
    "        if any(train[i].isna()):\n",
    "            # 对缺失值取均值处理\n",
    "            print(train[i].isna().sum())\n",
    "            train_i = train_i.replace(np.nan, train[i].mean())\n",
    "        if len(train_i.value_counts()) <= 20 and train_i.dtype != object:\n",
    "            train_i = train_i.astype(int)\n",
    "        train_[i] = train_i\n",
    "\n",
    "    return pd.DataFrame(train_)\n",
    "\n",
    "train = build_data(train)\n",
    "test = build_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cov = list(train.columns)\n",
    "# save data and their corresponding transformers\n",
    "class TransData:\n",
    "    def __init__(self, name, is_obj=False):\n",
    "        self.is_obj = is_obj\n",
    "        self.name = name\n",
    "        self.transformer = None\n",
    "\n",
    "    def __call__(self, data):\n",
    "        self.df = data[self.name]\n",
    "        series = self.df.to_numpy().reshape(-1, 1)\n",
    "        if self.df.dtype == object:\n",
    "            self.is_obj = True\n",
    "            self.transformer = OrdinalEncoder()\n",
    "            self.data = self.transformer.fit_transform(series).astype(int)\n",
    "        elif self.df.dtype != int:\n",
    "            self.transformer = StandardScaler()\n",
    "            self.data = self.transformer.fit_transform(series)\n",
    "        else:\n",
    "            self.data = series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V_8', 'V_10', 'V_14', 'V_26']\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "data_dict = {}\n",
    "cat_name = []\n",
    "test_dict = {}\n",
    "\n",
    "for name in all_cov:\n",
    "    t = TransData(name=name)\n",
    "    t(train)\n",
    "    data_dict[name] = t.data.reshape(-1, )\n",
    "    if t.is_obj:\n",
    "        cat_name.append(name)\n",
    "    if name not in ['treatment', 'outcome']:\n",
    "        try:\n",
    "            test_i = t.transformer.transform(test[name].values.reshape(-1, 1)).reshape(-1, )\n",
    "        except:\n",
    "            test_i = test[name]\n",
    "        test_dict[name] = test_i\n",
    "train_transformed = pd.DataFrame(data_dict)\n",
    "test_data = pd.DataFrame(test_dict)\n",
    "print(cat_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "V 特征列  x 干预方案 y 作用结果\n",
    "因果图已知 需根据数据分析 提取各特征列对应的影响变量\n",
    "构造模型\n",
    "实现因果预测的模型\n",
    "'''\n",
    "V = train_transformed.drop(['treatment', 'outcome'], axis=1).values\n",
    "x = train_transformed['treatment'].values\n",
    "y = train_transformed['outcome'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因果发现 \n",
    "CFdata = np.concatenate((V[:,35:40],x.reshape(-1,1),y.reshape(-1,1)),axis=1)\n",
    "\n",
    "# from causallearn.search.PermutationBased.GRaSP import grasp\n",
    "# G = grasp(X=CFdata,score_func='local_score_BIC', maxP=8 )\n",
    "# nodes = G.get_nodes()\n",
    "# print(len(nodes))\n",
    "\n",
    "\n",
    "# def get_G_parents(G, node):\n",
    "#     parent = G.get_parents(node)\n",
    "#     parents = []\n",
    "#     for i in parent:\n",
    "#         parents.append(i.get_name())\n",
    "#     return parents\n",
    "\n",
    "\n",
    "# node_tr = nodes[40]\n",
    "# parents_tr = get_G_parents(G,node_tr)\n",
    "# node_out = nodes[41]\n",
    "# parents_out = get_G_parents(G,node_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def x2V_in_parents(parents):\n",
    "#     new_parents = []\n",
    "#     for i in parents:\n",
    "#         new_parents.append(str(i).replace(\"x\",\"V_\"))\n",
    "#     return new_parents\n",
    "\n",
    "# grasp_confounder_list = x2V_in_parents(parents_tr)\n",
    "# grasp_convariate_list = x2V_in_parents(parents_out[:-1])\n",
    "# print(grasp_confounder_list,grasp_convariate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_confounder_list = ['V_1', 'V_2', 'V_3', 'V_11', 'V_18', 'V_33', 'V_35', 'V_37']\n",
    "\n",
    "grasp_convariate_list = ['V_1', 'V_2', 'V_3', 'V_4', 'V_9', 'V_10', 'V_11', 'V_14', 'V_16', 'V_17', 'V_18', 'V_19', 'V_21', 'V_22', 'V_28', 'V_29', 'V_30', 'V_31', 'V_32', 'V_33', 'V_35', 'V_36', 'V_39']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ce(data, x1_model, x2_model):\n",
    "    ce1 = x1_model.estimate(data)\n",
    "    ce2 = x2_model.estimate(data)\n",
    "    return np.concatenate([ce1.reshape(-1, 1), ce2.reshape(-1, 1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Application2\\anaconda\\anaconda\\envs\\ylearn-py38\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3977107\n",
      "0.7562739\n"
     ]
    }
   ],
   "source": [
    "from ylearn.estimator_model import TLearner, XLearner,SLearner\n",
    "from sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBRegressor(seed=1850,n_estimators=1000,learning_rate=0.1,\n",
    "                         max_depth=11)\n",
    "# model = RandomForestRegressor(n_estimators=150)\n",
    "# from model import Regressor\n",
    "# import imp\n",
    "# imp.reload(Regressor)\n",
    "# model = GradientBoostingRegressor(learning_rate=0.01, criterion=\"friedman_mse\",\n",
    "#                                              n_estimators=150, max_features=0.6)\n",
    "tl1 = XLearner(model=model)\n",
    "tl2 = XLearner(model=model)\n",
    "tl1.fit(data=train_transformed, treatment='treatment', outcome='outcome', treat=1,\n",
    "        control=0, covariate=grasp_confounder_list)\n",
    "tl2.fit(data=train_transformed, treatment='treatment', outcome='outcome', treat=2,\n",
    "        control=0, covariate=grasp_confounder_list)\n",
    "print(tl1.estimate(quantity=\"CATE\"))\n",
    "print(tl2.estimate(quantity=\"CATE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = get_ce(train_transformed, tl1, tl2)\n",
    "ce_test = get_ce(test_data, x1_model=tl1, x2_model=tl2)\n",
    "ce_total = np.concatenate((ce, ce_test), axis=0)\n",
    "# Save the result\n",
    "# ce2csv(ce_total,\"GraspConfounder_Xlearn_XGB_n1000_depth11.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ylearn-py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be2cb432b6f92c0fb2b967cc8c50d08b3407edd97e771d37af6dc4316c57763b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
