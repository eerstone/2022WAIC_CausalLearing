{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Application2\\anaconda\\anaconda\\envs\\ylearn-py38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "import ylearn\n",
    "from ylearn.causal_discovery import CausalDiscovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Demo\\PythonDemo\\2022WAIC_CausalLearing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# 项目根目录\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath('./baseline_example.ipynb')))\n",
    "# 添加系统环境变量\n",
    "print(BASE_DIR)\n",
    "sys.path.insert(0, BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(BASE_DIR+'/data/train.csv')\n",
    "test = pd.read_csv(BASE_DIR+'/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "25\n",
      "2\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# replace nan\n",
    "def build_data(train):\n",
    "    train_ = {}\n",
    "    for i in train.columns:\n",
    "        # 对每一序列进行预处理\n",
    "        # preprocessing for each series\n",
    "        train_i = train[i]\n",
    "        if any(train[i].isna()):\n",
    "            # 对缺失值取均值处理\n",
    "            print(train[i].isna().sum())\n",
    "            train_i = train_i.replace(np.nan, train[i].mean())\n",
    "        if len(train_i.value_counts()) <= 20 and train_i.dtype != object:\n",
    "            train_i = train_i.astype(int)\n",
    "        train_[i] = train_i\n",
    "\n",
    "    return pd.DataFrame(train_)\n",
    "\n",
    "train = build_data(train)\n",
    "test = build_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cov = list(train.columns)\n",
    "# save data and their corresponding transformers\n",
    "class TransData:\n",
    "    def __init__(self, name, is_obj=False):\n",
    "        self.is_obj = is_obj\n",
    "        self.name = name\n",
    "        self.transformer = None\n",
    "\n",
    "    def __call__(self, data):\n",
    "        self.df = data[self.name]\n",
    "        series = self.df.to_numpy().reshape(-1, 1)\n",
    "        if self.df.dtype == object:\n",
    "            self.is_obj = True\n",
    "            self.transformer = OrdinalEncoder()\n",
    "            self.data = self.transformer.fit_transform(series).astype(int)\n",
    "        elif self.df.dtype != int:\n",
    "            self.transformer = StandardScaler()\n",
    "            self.data = self.transformer.fit_transform(series)\n",
    "        else:\n",
    "            self.data = series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V_8', 'V_10', 'V_14', 'V_26']\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "data_dict = {}\n",
    "cat_name = []\n",
    "test_dict = {}\n",
    "\n",
    "for name in all_cov:\n",
    "    t = TransData(name=name)\n",
    "    t(train)\n",
    "    data_dict[name] = t.data.reshape(-1, )\n",
    "    if t.is_obj:\n",
    "        cat_name.append(name)\n",
    "    if name not in ['treatment', 'outcome']:\n",
    "        try:\n",
    "            test_i = t.transformer.transform(test[name].values.reshape(-1, 1)).reshape(-1, )\n",
    "        except:\n",
    "            test_i = test[name]\n",
    "        test_dict[name] = test_i\n",
    "train_transformed = pd.DataFrame(data_dict)\n",
    "test_data = pd.DataFrame(test_dict)\n",
    "print(cat_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V_0</th>\n",
       "      <th>V_1</th>\n",
       "      <th>V_2</th>\n",
       "      <th>V_3</th>\n",
       "      <th>V_4</th>\n",
       "      <th>V_5</th>\n",
       "      <th>V_6</th>\n",
       "      <th>V_7</th>\n",
       "      <th>V_8</th>\n",
       "      <th>V_9</th>\n",
       "      <th>...</th>\n",
       "      <th>V_32</th>\n",
       "      <th>V_33</th>\n",
       "      <th>V_34</th>\n",
       "      <th>V_35</th>\n",
       "      <th>V_36</th>\n",
       "      <th>V_37</th>\n",
       "      <th>V_38</th>\n",
       "      <th>V_39</th>\n",
       "      <th>treatment</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.723577</td>\n",
       "      <td>-0.305753</td>\n",
       "      <td>-0.713223</td>\n",
       "      <td>-1.621706</td>\n",
       "      <td>-0.110603</td>\n",
       "      <td>0</td>\n",
       "      <td>1.967215</td>\n",
       "      <td>-1.605903</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.175898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.983957</td>\n",
       "      <td>1.170614</td>\n",
       "      <td>-0.043524</td>\n",
       "      <td>1.491432</td>\n",
       "      <td>1.246007</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.965484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.620006</td>\n",
       "      <td>1.144513</td>\n",
       "      <td>-0.713223</td>\n",
       "      <td>-0.836881</td>\n",
       "      <td>-0.329293</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.321160</td>\n",
       "      <td>0.287543</td>\n",
       "      <td>0</td>\n",
       "      <td>0.193801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.935753</td>\n",
       "      <td>0.229336</td>\n",
       "      <td>0.849727</td>\n",
       "      <td>0.005753</td>\n",
       "      <td>0.958077</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.110879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.844489</td>\n",
       "      <td>0.105237</td>\n",
       "      <td>1.239680</td>\n",
       "      <td>-1.558425</td>\n",
       "      <td>-0.300993</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.277983</td>\n",
       "      <td>0.717924</td>\n",
       "      <td>0</td>\n",
       "      <td>0.193801</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.043339</td>\n",
       "      <td>-0.713962</td>\n",
       "      <td>-0.861334</td>\n",
       "      <td>0.631476</td>\n",
       "      <td>-0.289619</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.258860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.218723</td>\n",
       "      <td>-0.367827</td>\n",
       "      <td>-0.713223</td>\n",
       "      <td>-1.575069</td>\n",
       "      <td>-0.870663</td>\n",
       "      <td>1</td>\n",
       "      <td>0.952558</td>\n",
       "      <td>0.775616</td>\n",
       "      <td>0</td>\n",
       "      <td>0.193801</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.358267</td>\n",
       "      <td>0.035055</td>\n",
       "      <td>0.845040</td>\n",
       "      <td>0.112702</td>\n",
       "      <td>-0.481573</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.267371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.183640</td>\n",
       "      <td>0.928402</td>\n",
       "      <td>-0.713223</td>\n",
       "      <td>-0.134138</td>\n",
       "      <td>0.654154</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.472279</td>\n",
       "      <td>0.776770</td>\n",
       "      <td>0</td>\n",
       "      <td>0.193801</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078760</td>\n",
       "      <td>-0.046988</td>\n",
       "      <td>-0.110786</td>\n",
       "      <td>0.682046</td>\n",
       "      <td>1.725891</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.166405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        V_0       V_1       V_2       V_3       V_4  V_5       V_6       V_7  \\\n",
       "0  1.723577 -0.305753 -0.713223 -1.621706 -0.110603    0  1.967215 -1.605903   \n",
       "1 -0.620006  1.144513 -0.713223 -0.836881 -0.329293    0 -0.321160  0.287543   \n",
       "2 -0.844489  0.105237  1.239680 -1.558425 -0.300993    1 -0.277983  0.717924   \n",
       "3  0.218723 -0.367827 -0.713223 -1.575069 -0.870663    1  0.952558  0.775616   \n",
       "4  0.183640  0.928402 -0.713223 -0.134138  0.654154    1 -0.472279  0.776770   \n",
       "\n",
       "   V_8       V_9  ...      V_32      V_33      V_34      V_35      V_36  V_37  \\\n",
       "0    0 -5.175898  ...  0.983957  1.170614 -0.043524  1.491432  1.246007    -2   \n",
       "1    0  0.193801  ...  0.935753  0.229336  0.849727  0.005753  0.958077     0   \n",
       "2    0  0.193801  ... -2.043339 -0.713962 -0.861334  0.631476 -0.289619     1   \n",
       "3    0  0.193801  ... -0.358267  0.035055  0.845040  0.112702 -0.481573     1   \n",
       "4    0  0.193801  ... -0.078760 -0.046988 -0.110786  0.682046  1.725891     1   \n",
       "\n",
       "   V_38  V_39  treatment   outcome  \n",
       "0     0     3          2  0.965484  \n",
       "1     2     4          0  1.110879  \n",
       "2     1     2          2 -2.258860  \n",
       "3     0     3          0 -0.267371  \n",
       "4     0     2          2 -0.166405  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "V 特征列  x 干预方案 y 作用结果\n",
    "因果图已知 需根据数据分析 提取各特征列对应的影响变量\n",
    "构造模型\n",
    "实现因果预测的模型\n",
    "'''\n",
    "V = train_transformed.drop(['treatment', 'outcome'], axis=1).values\n",
    "x = train_transformed['treatment'].values\n",
    "y = train_transformed['outcome'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(0, [2, 4, 5, 6]), (1, [2, 4, 5, 6]), (2, [4, 6]), (3, [4, 5, 6]), (4, [6]), (5, [2, 4, 6]), (6, [])])\n"
     ]
    }
   ],
   "source": [
    "# 因果发现 Ylearn自带的没啥用好像 计算速度太慢\n",
    "slice = np.concatenate((V[:,:5],x.reshape(-1,1),y.reshape(-1,1)),axis=1)\n",
    "cd = CausalDiscovery(hidden_layer_dim=[3])\n",
    "est = cd(slice, threshold=0.01, return_dict=True)\n",
    "print(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0 -1  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]]\n",
      "[[ 0  0  0  0  0 -1  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0 -1]\n",
      " [ 0  0  0  0  0  1  0]]\n",
      "[[ 0  0  0  0  0 -1  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0 -1]\n",
      " [ 0  0  1  0  0  1  0]]\n",
      "[[ 0  0  0  0  0 -1  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1]\n",
      " [-1  0  0  0  0  0 -1]\n",
      " [ 0  0  1  0  1  1  0]]\n",
      "[[ 0  0  0  0  0 -1  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1 -1]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1]\n",
      " [ 1  0  1  0  0  0 -1]\n",
      " [ 0  0  1  0  1  1  0]]\n",
      "[[ 0  0  0  0  0 -1  0]\n",
      " [ 0  0  0  0  0  0 -1]\n",
      " [ 0  0  0  0  0 -1 -1]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1]\n",
      " [ 1  0  1  0  0  0 -1]\n",
      " [ 0  1  1  0  1  1  0]]\n",
      "[[ 0  0  0  0  0 -1  0]\n",
      " [ 0  0  0  0  0  0 -1]\n",
      " [ 0  0  0  0 -1 -1 -1]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  0 -1]\n",
      " [ 1  0  1  0  0  0 -1]\n",
      " [ 0  1  1  0  1  1  0]]\n",
      "backward\n"
     ]
    }
   ],
   "source": [
    "# 因果发现 \n",
    "CFdata = np.concatenate((V[:,35:40],x.reshape(-1,1),y.reshape(-1,1)),axis=1)\n",
    "\n",
    "from causallearn.search.ScoreBased.GES import ges\n",
    "Record = ges(CFdata)\n",
    "G = Record['G']\n",
    "# from causallearn.search.ConstraintBased.PC import pc\n",
    "# G = pc(np.concatenate((V[:,0:20],x.reshape(-1,1),y.reshape(-1,1)),axis=1))\n",
    "# # visualization using pydot\n",
    "# cg.draw_pydot_graph()\n",
    "\n",
    "# from causallearn.search.ConstraintBased.FCI import fci\n",
    "# G, edges = fci(CFdata)\n",
    "\n",
    "# # visualization\n",
    "# from causallearn.utils.GraphUtils import GraphUtils\n",
    "# pdy = GraphUtils.to_pydot(G)\n",
    "# pdy.write_png('simple_test_30-40.png')\n",
    "\n",
    "# from causallearn.search.PermutationBased.GRaSP import grasp\n",
    "# G = grasp(X=CFdata,score_func='local_score_BIC', maxP=8 )\n",
    "\n",
    "# # Visualization using pydot\n",
    "# from causallearn.utils.GraphUtils import GraphUtils\n",
    "# import matplotlib.image as mpimg\n",
    "# import matplotlib.pyplot as plt\n",
    "# import io\n",
    "\n",
    "# pyd = GraphUtils.to_pydot(G)\n",
    "# tmp_png = pyd.create_png(f=\"png\")\n",
    "# fp = io.BytesIO(tmp_png)\n",
    "# img = mpimg.imread(fp, format='png')\n",
    "# plt.axis('off')\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "['x0', 'x2']\n",
      "['x1', 'x2', 'x4', 'x5']\n"
     ]
    }
   ],
   "source": [
    "nodes = G.get_nodes()\n",
    "print(len(nodes))\n",
    "\n",
    "def get_G_parents(G, node):\n",
    "    parent = G.get_parents(node)\n",
    "    parents = []\n",
    "    for i in parent:\n",
    "        parents.append(i.get_name())\n",
    "    return parents\n",
    "\n",
    "node_tr = nodes[5]\n",
    "parents_tr = get_G_parents(G,node_tr)\n",
    "node_out = nodes[6]\n",
    "parents_out = get_G_parents(G,node_out)\n",
    "print(parents_tr)\n",
    "print(parents_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ges_confounderlist = ['V_1', 'V_2', 'V_3', 'V_6', 'V_7', 'V_11', 'V_13', 'V_18'] + ['V_31','V_33','V_35','V_37']\n",
    "\n",
    "score_ges_convariatelist = ['V_2', 'V_5', 'V_6', 'V_7', 'V_8', 'V_9', 'V_10', 'V_14', 'V_15', 'V_16', 'V_19'] + ['V_31','V_33','V_36','V_37','V_39']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V_1', 'V_2', 'V_3', 'V_6', 'V_7', 'V_11', 'V_13', 'V_18'] ['V_2', 'V_5', 'V_6', 'V_7', 'V_8', 'V_9', 'V_10', 'V_14', 'V_15', 'V_16', 'V_19']\n"
     ]
    }
   ],
   "source": [
    "def x2V_in_parents(parents):\n",
    "    new_parents = []\n",
    "    for i in parents:\n",
    "        new_parents.append(str(i).replace(\"x\",\"V_\"))\n",
    "    return new_parents\n",
    "\n",
    "grasp_confounder_list = x2V_in_parents(parents_tr)\n",
    "grasp_convariate_list = x2V_in_parents(parents_out[:-1])\n",
    "print(grasp_confounder_list,grasp_convariate_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grasp_confounder_list = ['V_1', 'V_2', 'V_3', 'V_11', 'V_18', 'V_33', 'V_35', 'V_37']\n",
    "\n",
    "grasp_convariate_list = ['V_1', 'V_2', 'V_3', 'V_4', 'V_9', 'V_10', 'V_11', 'V_14', 'V_16', 'V_17', 'V_18', 'V_19', 'V_21', 'V_22', 'V_28', 'V_29', 'V_30', 'V_31', 'V_32', 'V_33', 'V_35', 'V_36', 'V_39']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "训练xmodel的分类器，与ymodel的回归模型\n",
    "'''\n",
    "x_model = RandomForestClassifier(n_estimators=150, criterion='entropy', max_features=0.5, max_depth=50)\n",
    "y_model = RandomForestRegressor(n_estimators=150, max_features=0.5, max_depth=100, )\n",
    "x_model.fit(V, x)\n",
    "x_importance = x_model.feature_importances_\n",
    "y_model_input = np.concatenate((V, x.reshape(-1, 1)), axis=1)\n",
    "y_model.fit(y_model_input, y=y)\n",
    "y_importance = y_model.feature_importances_\n",
    "convariate_list = []\n",
    "for i, (x_, y_) in enumerate(zip(x_importance, y_importance)):\n",
    "    if x_ >= 1e-3 and y_ >= 1e-5:\n",
    "        convariate_list.append(all_cov[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V_0', 'V_1', 'V_2', 'V_3', 'V_4', 'V_5', 'V_6', 'V_7', 'V_10', 'V_11', 'V_12', 'V_13', 'V_14', 'V_15', 'V_17', 'V_18', 'V_19', 'V_21', 'V_22', 'V_23', 'V_24', 'V_25', 'V_28', 'V_29', 'V_30', 'V_31', 'V_32', 'V_33', 'V_34', 'V_35', 'V_36', 'V_37', 'V_38', 'V_39']\n"
     ]
    }
   ],
   "source": [
    "# 只考虑有意义的混杂变量\n",
    "V_new = train_transformed[convariate_list + ['treatment'] + ['outcome']]\n",
    "print(convariate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V_0', 'V_2', 'V_10', 'V_11', 'V_12', 'V_15', 'V_17', 'V_18', 'V_19', 'V_21', 'V_22', 'V_23', 'V_24', 'V_25', 'V_28', 'V_29', 'V_30', 'V_31', 'V_32', 'V_33', 'V_37', 'V_38', 'V_39']\n"
     ]
    }
   ],
   "source": [
    "# 重定义混淆因子\n",
    "confounder_list = ['V_1','V_3','V_4','V_5','V_7','V_6','V_13','V_14','V_34','V_35','V_36']\n",
    "# confounder_list = ['V_1','V_13','V_14']\n",
    "for x in confounder_list:\n",
    "    convariate_list.remove(x) \n",
    "print(convariate_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce2csv(ce_total, file_name=\"result.csv\"):\n",
    "    pd.DataFrame(ce_total).to_csv(BASE_DIR + \"/results/\" + file_name,\n",
    "                              index=False,\n",
    "                              header=['ce_1', 'ce_2'])\n",
    "    return \n",
    "\n",
    "\n",
    "# 评估训练集得分\n",
    "def get_nrmse(predictions, targets):\n",
    "    differences = predictions - targets\n",
    "    differences_square = differences**2\n",
    "    mean_of_differences_square = differences_square.mean()\n",
    "    rmse = np.sqrt(mean_of_differences_square)\n",
    "    nrmse = rmse / (targets.mean())\n",
    "    return np.abs(nrmse)\n",
    "\n",
    "def ce_score(ce, V_new):\n",
    "    '''\n",
    "    !!!  无效，评分为因果效应评分，并非直接减去outcome\n",
    "    '''\n",
    "    TREATMENT = 1\n",
    "    mask = V_new['treatment']== TREATMENT\n",
    "    train_1_nrmse = get_nrmse(ce[mask, TREATMENT-1], V_new['outcome'][mask])\n",
    "    TREATMENT = 2\n",
    "    mask = V_new['treatment']== TREATMENT\n",
    "    train_2_nrmse = get_nrmse(ce[mask, TREATMENT-1], V_new['outcome'][mask])\n",
    "    print(train_1_nrmse,train_2_nrmse)\n",
    "    return train_1_nrmse,train_2_nrmse\n",
    "\n",
    "\n",
    "def get_ce(data, x1_model, x2_model):\n",
    "    ce1 = x1_model.estimate(data)\n",
    "    ce2 = x2_model.estimate(data)\n",
    "    return np.concatenate([ce1.reshape(-1, 1), ce2.reshape(-1, 1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "treatment\n",
       "0   -0.302371\n",
       "1   -0.381580\n",
       "2    0.236624\n",
       "Name: outcome, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transformed.groupby('treatment').outcome.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3977107\n",
      "0.7562739\n"
     ]
    }
   ],
   "source": [
    "from ylearn.estimator_model import TLearner, XLearner,SLearner\n",
    "from sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBRegressor(seed=1850,n_estimators=1000,learning_rate=0.1,\n",
    "                         max_depth=11)\n",
    "# model = RandomForestRegressor(n_estimators=150)\n",
    "# from model import Regressor\n",
    "# import imp\n",
    "# imp.reload(Regressor)\n",
    "# model = GradientBoostingRegressor(learning_rate=0.01, criterion=\"friedman_mse\",\n",
    "#                                              n_estimators=150, max_features=0.6)\n",
    "tl1 = XLearner(model=model)\n",
    "tl2 = XLearner(model=model)\n",
    "tl1.fit(data=train_transformed, treatment='treatment', outcome='outcome', treat=1,\n",
    "        control=0, covariate=grasp_confounder_list)\n",
    "tl2.fit(data=train_transformed, treatment='treatment', outcome='outcome', treat=2,\n",
    "        control=0, covariate=grasp_confounder_list)\n",
    "print(tl1.estimate(quantity=\"CATE\"))\n",
    "print(tl2.estimate(quantity=\"CATE\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ce是训练集（train.csv）上的因果效应，另外需要估计测试集（test.csv）上的因果效应ce_test。最后需要把ce_test拼接在ce之后，存储到一个csv文件中上传到平台取得得分。得分为一个数值，越小说明结果越接近真实值，得分最小值为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = get_ce(train_transformed, tl1, tl2)\n",
    "ce_test = get_ce(test_data, x1_model=tl1, x2_model=tl2)\n",
    "ce_total = np.concatenate((ce, ce_test), axis=0)\n",
    "ce2csv(ce_total,\"GraspConfounder_Xlearn_XGB_n1000_depth11.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下内容是给出的其他两种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ylearn.estimator_model import DML4CATE\n",
    "\n",
    "dml = DML4CATE(cf_fold=1, x_model=RandomForestClassifier(n_estimators=250, criterion=\"entropy\", max_depth=150, min_samples_leaf=2, min_samples_split=3, max_features=3),\n",
    "               y_model=RandomForestRegressor(n_estimators=250, max_depth=150, min_samples_leaf=2, min_samples_split=2, max_features=3), is_discrete_treatment=True)\n",
    "dml.fit(data=V_new, outcome='outcome', treatment='treatment', covariate=confounder_list,)\n",
    "ce_dml = dml.effect_nji(data=V_new, control=0)\n",
    "ce_dml_test = dml.effect_nji(data=test_data, control=0)\n",
    "ce_dml_train = ce_dml[:, :, 1:].reshape(-1, 2)\n",
    "ce_dml_all = np.concatenate([ce_dml_train, ce_dml_test[:, :, 1:].reshape(-1, 2)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalTree(max_depth=100, max_features=20)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ylearn.estimator_model import CausalTree\n",
    "\n",
    "ct1 = CausalTree(max_depth=100, min_samples_split=2, max_features=20)\n",
    "ct2 = CausalTree(max_depth=100, max_features=20)\n",
    "ct1.fit(data=V_new, outcome='outcome', treatment='treatment',adjustment=confounder_list, \n",
    "                covariate=convariate_list, treat=[1], control=[0])\n",
    "ct2.fit(data=V_new, outcome='outcome', treatment='treatment',adjustment=confounder_list, \n",
    "                covariate=convariate_list, treat=[2], control=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_ct = get_ce(V_new, ct1, ct2)\n",
    "ce_ct_test = get_ce(test_data, ct1, ct2)\n",
    "ce_ct_all = np.concatenate([ce_ct, ce_ct_test], axis=0)\n",
    "ce2csv(ce_ct_all,\"CausalTree.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DoublyRobust(x_model=RandomForestClassifier(max_depth=100, min_samples_leaf=361), y_model=GradientBoostingRegressor(max_depth=100, min_samples_leaf=361), yx_model=GradientBoostingRegressor(max_depth=100, min_samples_leaf=361), random_state=2022)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ylearn.estimator_model.doubly_robust import DoublyRobust\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "n = V_new.shape[0]\n",
    "dr = DoublyRobust(\n",
    "    x_model=RandomForestClassifier(n_estimators=100, max_depth=100, min_samples_leaf=int(n/100)),\n",
    "    y_model=GradientBoostingRegressor(n_estimators=100, max_depth=100, min_samples_leaf=int(n/100)),\n",
    "    yx_model=GradientBoostingRegressor(n_estimators=100, max_depth=100, min_samples_leaf=int(n/100)),\n",
    "    cf_fold=1,\n",
    "    random_state=2022,\n",
    ")\n",
    "dr.fit(data=V_new, outcome=\"outcome\", treatment=\"treatment\",control=0, adjustment=confounder_list, covariate=convariate_list,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_pred_2 = dr.estimate(data=test_data, all_tr_effects=True).reshape(-1,3)\n",
    "dr_pred_1 = dr.estimate(data=V_new, all_tr_effects=True).reshape(-1,3)\n",
    "dr_ce_1 = dr_pred_1[:,1:]\n",
    "dr_ce_2 = dr_pred_2[:,1:]\n",
    "dr_ce_total = np.concatenate((dr_ce_1,dr_ce_2),axis=0)\n",
    "ce2csv(dr_ce_total,\"handcrafted_DoublyRobust2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ylearn-py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be2cb432b6f92c0fb2b967cc8c50d08b3407edd97e771d37af6dc4316c57763b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
